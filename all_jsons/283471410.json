{"cited_in": [], "datas": {"publication_uid": "283471410", "title": "FireCaffe: near-linear acceleration of deep neural network training on compute clusters", "authors": {"1": "Forrest N. Iandola", "4": "Kurt Keutzer", "3": "Mattthew W. Moskewicz", "2": "Khalid Ashraf"}, "abstract": "  ABSTRACT Long training times for high-accuracy deep neural networks (DNNs) impede research into new DNN architectures and slow the development of high-accuracy DNNs. In this paper we present FireCaffe, which successfully scales deep neural network training across a cluster of GPUs. We also present a number of best practices to aid in comparing advancements in methods for scaling and accelerating the training of deep neural networks. The speed and scalability of distributed algorithms is almost always limited by the overhead of communicating between servers; DNN training is not an exception to this rule. Therefore, the key consideration here is to reduce communication overhead wherever possible, while not degrading the accuracy of the DNN models that we train. Our approach has three key pillars. First, we select network hardware that achieves high bandwidth between GPU servers -- Infiniband or Cray interconnects are ideal for this. Second, we consider a number of communication algorithms, and we find that reduction trees are more efficient and scalable than the traditional parameter server approach. Third, we optionally increase the batch size to reduce the total quantity of communication during DNN training, and we identify hyperparameters that allow us to reproduce the small-batch accuracy while training with large batch sizes. When training GoogLeNet and Network-in-Network on ImageNet, we achieve a 16x and 23x speedup, respectively, when training on a cluster of 32 GPUs.  "}, "references": ["https://www.researchgate.net/publication/266560893_cuDNN_Efficient_Primitives_for_Deep_Learning", "https://www.researchgate.net/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database", "https://www.researchgate.net/publication/282402796_A_Deep_Neural_Network_Compression_Pipeline_Pruning_Quantization_Huffman_Encoding", "https://www.researchgate.net/publication/272194743_Batch_Normalization_Accelerating_Deep_Network_Training_by_Reducing_Internal_Covariate_Shift", "https://www.researchgate.net/publication/262380602_Speeding_up_Convolutional_Neural_Networks_with_Low_Rank_Expansions", "https://www.researchgate.net/publication/261880160_One_weird_trick_for_parallelizing_convolutional_neural_networks", "https://www.researchgate.net/publication/274572264_End-to-End_Training_of_Deep_Visuomotor_Policies", "https://www.researchgate.net/publication/257882504_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality", "https://www.researchgate.net/publication/265385906_Very_Deep_Convolutional_Networks_for_Large-Scale_Image_Recognition", "https://www.researchgate.net/publication/224581828_Benchmarking_GPUs_to_tune_dense_linear_algebra", "https://www.researchgate.net/publication/259399749_Single_Server_Multi-GPU_Training_of_ConvNets", "https://www.researchgate.net/publication/261440096_DenseNet_Implementing_Efficient_ConvNet_Descriptor_Pyramids", "https://www.researchgate.net/publication/267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks", "https://www.researchgate.net/publication/51951195_HOGWILD_A_Lock-Free_Approach_to_Parallelizing_Stochastic_GradientDescent", "https://www.researchgate.net/publication/268525230_From_Captions_to_Visual_Concepts_and_Back", "https://www.researchgate.net/publication/265787949_Going_Deeper_with_Convolutions", "https://www.researchgate.net/publication/266528941_MDB_A_Memory-Mapped_Database_and_Backend_for_OpenLDAP", "https://www.researchgate.net/publication/2954554_Spert-II_A_Vector_microprocessor_system", "https://www.researchgate.net/publication/271532741_maxDNN_An_Efficient_Convolution_Kernel_for_Deep_Learning_with_Maxwell_GPUs", "https://www.researchgate.net/publication/264979485_Caffe_Convolutional_Architecture_for_Fast_Feature_Embedding", "https://www.researchgate.net/publication/269994818_Fast_Convolutional_Nets_With_fbfft_A_GPU_Performance_Evaluation", "https://www.researchgate.net/publication/263352539_From_Generic_to_Specific_Deep_Representations_for_Visual_Recognition", "https://www.researchgate.net/publication/270906413_Deep_Image_Scaling_up_Image_Recognition", "https://www.researchgate.net/publication/257368116_Communication-Minimizing_2D_Convolution_in_GPU_Registers", "https://www.researchgate.net/publication/282691979_DeepLogo_Hitting_Logo_Recognition_with_the_Deep_Neural_Network_Hammer", "https://www.researchgate.net/publication/268451925_Efficient_and_Accurate_Approximations_of_Nonlinear_Convolutional_Networks", "https://www.researchgate.net/publication/269339627_Theano-based_Large-Scale_Visual_Recognition_with_Multiple_GPUs", "https://www.researchgate.net/publication/215616968_Understanding_the_difficulty_of_training_deep_feedforward_neural_networks", "https://www.researchgate.net/publication/220457366_Optimization_of_Collective_Communication_Operations_in_MPICH", "https://www.researchgate.net/publication/275974753_Empirical_Evaluation_of_Rectified_Activations_in_Convolutional_Network", "https://www.researchgate.net/publication/221487297_Conversational_Speech_Transcription_Using_Context-Dependent_Deep_Neural_Networks", "https://www.researchgate.net/publication/269722411_DeepSpeech_Scaling_up_end-to-end_speech_recognition"]}