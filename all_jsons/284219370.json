{"cited_in": [], "datas": {"publication_uid": "284219370", "title": "Why are deep nets reversible: A simple theory, with implications for training", "authors": {"1": "Sanjeev Arora", "3": "Tengyu Ma", "2": "Yingyu Liang"}, "abstract": "  ABSTRACT Generative model approaches to deep learning are of interest in the quest for both better understanding as well as training methods requiring fewer labeled samples. Recent works use generative model approaches to produce the deep net's input given the value of a hidden layer several levels above. However, there is no accompanying \"proof of correctness,\" for the generative model, showing that the feedforward deep net is the correct inference method for recovering the hidden layer given the input. Furthermore, these models are complicated. The current paper takes a more {\\em theoretical} tack. It presents a very simple generative model for RELU deep nets, with the following characteristics: (a) The generative model is just the {\\em reverse} of the feedforward net: if the forward transformation at a layer is $A$ then the reverse transformation is $A^T$. (This can be seen as an explanation of the old {\\em weight tying} method for denoising autoencoders.) (b) Its correctness can be {\\em proven} under a clean theoretical assumption: the edge weights in real-life deep nets behave like random numbers. Under this assumption ---which is experimentally tested on real-life nets like AlexNet--- it is formally proved that feed forward net is a correct inference method for recovering the hidden layer. (c) The generative model suggests a simple modification for training---use an input to produce several synthetic inputs with the same label, and include them in the backprop training. This appears to yield benefits similar to dropout, and can also be seen as a generative explanation for the efficacy of dropout.  "}, "references": ["https://www.researchgate.net/publication/258083111_Provable_Bounds_for_Learning_Some_Deep_Representations", "https://www.researchgate.net/publication/236955042_Generalized_Denoising_Auto-Encoders_as_Generative_Models", "https://www.researchgate.net/publication/6912170_Reducing_the_Dimensionality_of_Data_with_Neural_Networks", "https://www.researchgate.net/publication/7017915_A_Fast_Learning_Algorithm_for_Deep_Belief_Nets", "https://www.researchgate.net/publication/264979485_Caffe_Convolutional_Architecture_for_Fast_Feature_Embedding", "https://www.researchgate.net/publication/263316327_Semi-Supervised_Learning_with_Deep_Generative_Models", "https://www.researchgate.net/publication/267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks", "https://www.researchgate.net/publication/2985446_Gradient-based_learning_applied_to_document_recognition_Proc_IEEE", "https://www.researchgate.net/publication/220424626_Unsupervised_Learning_of_Hierarchical_Representations_with_Convolutional_Deep_Belief_Networks", "https://www.researchgate.net/publication/269040983_Understanding_Deep_Image_Representations_by_Inverting_Them", "https://www.researchgate.net/publication/221345737_Rectified_Linear_Units_Improve_Restricted_Boltzmann_Machines_Vinod_Nair", "https://www.researchgate.net/publication/2539230_On_Discriminative_vs_Generative_Classifiers_A_comparison_of_logistic_regression_and_naive_Bayes", "https://www.researchgate.net/publication/274402963_A_Probabilistic_Theory_of_Deep_Learning", "https://www.researchgate.net/publication/221346269_Extracting_and_composing_robust_features_with_denoising_autoencoders", "https://www.researchgate.net/publication/220320676_Stacked_Denoising_Autoencoders_Learning_Useful_Representations_in_a_Deep_Network_with_a_Local_Denoising_Criterion", "https://www.researchgate.net/publication/273388040_Deep_Learning_and_the_Information_Bottleneck_Principle", "https://www.researchgate.net/publication/2821917_Unsupervised_Learning_of_Distributions_on_Binary_Vectors_Using_Two_Layer_Networks", "https://www.researchgate.net/publication/277959140_Path-SGD_Path-Normalized_Optimization_in_Deep_Neural_Networks", "https://www.researchgate.net/publication/265748773_Learning_Multiple_Layers_of_Features_from_Tiny_Images", "https://www.researchgate.net/publication/268155578_Deep_Exponential_Families", "https://www.researchgate.net/publication/237053970_Deep_Generative_Stochastic_Networks_Trainable_by_Backprop", "https://www.researchgate.net/publication/263012109_Generative_Adversarial_Networks", "https://www.researchgate.net/publication/215991023_Learning_Deep_Architectures_for_AI", "https://www.researchgate.net/publication/200744514_Greedy_layer-wise_training_of_deep_networks", "https://www.researchgate.net/publication/215992186_Supporting_Online_Material_for_Reducing_the_Dimensionality_of_Data_with_Neural_Networks"]}