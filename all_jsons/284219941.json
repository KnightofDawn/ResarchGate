{"cited_in": [], "datas": {"publication_uid": "284219941", "title": "Yin and Yang: Balancing and Answering Binary Visual Questions", "authors": {"1": "Peng Zhang", "4": "Dhruv Batra", "3": "Douglas Summers-Stay", "2": "Yash Goyal", "5": "Devi Parikh"}, "abstract": "The complex compositional structure of language makes problems at the intersection of vision and language challenging. But language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content. This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI. In this paper, we address binary Visual Question Answering (VQA) on abstract scenes. We formulate this problem as visual verification of concepts inquired in the questions. Specifically, we convert the question to a tuple that concisely summarizes the visual concept to be detected in the image. If the concept can be found in the image, the answer to the question is yes, and otherwise no. Abstract scenes play two roles (1) They allow us to focus on the high-level semantics of the VQA task as opposed to the low-level recognition problems, and perhaps more importantly, (2) They provide us the modality to balance the dataset such that language priors are controlled, and the role of vision is essential. In particular, we collect pairs of scenes for every question, such that the answer to the question is \"yes\" for one scene, and \"no\" for the other for the exact same question. Indeed, language priors alone do not perform better than chance on our balanced dataset. Moreover, our proposed approach outperforms a state-of-the-art VQA system on both balanced and unbalanced datasets."}, "references": ["https://www.researchgate.net/publication/275897099_VQA_Visual_Question_Answering", "https://www.researchgate.net/publication/220877439_VizWiz_Nearly_Real-time_Answers_to_Visual_Questions", "https://www.researchgate.net/publication/261227187_Simultaneous_Active_Learning_of_Classifiers_Attributes_via_Relative_Feedback", "https://www.researchgate.net/publication/268524772_Learning_a_Recurrent_Visual_Representation_for_Image_Caption_Generation", "https://www.researchgate.net/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database", "https://www.researchgate.net/publication/276923068_Exploring_Nearest_Neighbor_Approaches_for_Image_Captioning", "https://www.researchgate.net/publication/221110391_Annotator_rationales_for_visual_recognition", "https://www.researchgate.net/publication/268525836_Long-term_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description", "https://www.researchgate.net/publication/221012950_Identifying_Relations_for_Open_Information_Extraction", "https://www.researchgate.net/publication/270877442_Paraphrase-Driven_Learning_for_Open_Question_Answering", "https://www.researchgate.net/publication/266660267_Open_question_answering_over_curated_and_extracted_knowledge_bases", "https://www.researchgate.net/publication/268525230_From_Captions_to_Visual_Concepts_and_Back", "https://www.researchgate.net/publication/277023024_Are_You_Talking_to_a_Machine_Dataset_and_Methods_for_Multilingual_Image_Question_Answering", "https://www.researchgate.net/publication/273387445_Visual_Turing_test_for_computer_vision_systems", "https://www.researchgate.net/publication/228524009_HunPos_an_open_source_trigram_tagger", "https://www.researchgate.net/publication/259367619_Some_Improvements_on_Deep_Convolutional_Neural_Network_Based_Image_Classification", "https://www.researchgate.net/publication/269339562_Deep_Visual-Semantic_Alignments_for_Generating_Image_Descriptions", "https://www.researchgate.net/publication/268155634_Unifying_Visual-Semantic_Embeddings_with_Multimodal_Neural_Language_Models", "https://www.researchgate.net/publication/272752159_Don't_Just_Listen_Use_Your_Imagination_Leveraging_Visual_Common_Sense_for_Non-Visual_Tasks", "https://www.researchgate.net/publication/266376838_A_Multi-World_Approach_to_Question_Answering_about_Real-World_Scenes_based_on_Uncertain_Input", "https://www.researchgate.net/publication/275974823_Ask_Your_Neurons_A_Neural-based_Approach_to_Answering_Questions_about_Images", "https://www.researchgate.net/publication/262409300_Attributes_for_Classifier_Feedback", "https://www.researchgate.net/publication/220860992_Best_Practices_for_Convolutional_Neural_Networks_Applied_to_Visual_Document_Analysis", "https://www.researchgate.net/publication/268451983_Show_and_Tell_A_Neural_Image_Caption_Generator", "https://www.researchgate.net/publication/272522139_Towards_AI-Complete_Question_Answering_A_Set_of_Prerequisite_Toy_Tasks", "https://www.researchgate.net/publication/220817184_Using_Annotator_Rationales_to_Improve_Machine_Learning_for_Text_Categorization", "https://www.researchgate.net/publication/261236786_Bringing_Semantics_Into_Focus_Using_Visual_Abstraction", "https://www.researchgate.net/publication/271551178_Learning_the_Visual_Interpretation_of_Sentences"]}