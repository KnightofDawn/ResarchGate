{"cited_in": ["https://www.researchgate.net/publication/284219144_Learning_Deep_Structure-Preserving_Image-Text_Embeddings", "https://www.researchgate.net/publication/284097028_Sherlock_Modeling_Structured_Knowledge_in_Images", "https://www.researchgate.net/publication/283761954_Visual7W_Grounded_Question_Answering_in_Images", "https://www.researchgate.net/publication/283761587_Grounding_of_Textual_Phrases_in_Images_by_Reconstruction", "https://www.researchgate.net/publication/283658912_Generation_and_Comprehension_of_Unambiguous_Object_Descriptions", "https://www.researchgate.net/publication/275364608_Multimodal_Convolutional_Neural_Networks_for_Matching_Image_and_Sentence", "https://www.researchgate.net/publication/284218871_Order-Embeddings_of_Images_and_Language"], "datas": {"publication_uid": "277023758", "title": "Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models", "authors": {"2": "Liwei Wang", "6": "Svetlana Lazebnik", "1": "Bryan A. Plummer", "4": "Juan C. Caicedo", "3": "Chris M. Cervantes", "5": "Julia Hockenmaier"}, "abstract": "  ABSTRACT The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains linking mentions of the same entities in images, as well as 276k manually annotated bounding boxes corresponding to each entity. Such annotation is essential for continued progress in automatic image description and grounded language understanding. We present experiments demonstrating the usefulness of our annotations for bidirectional image-sentence retrieval and text-to-image coreference. These experiments confirm that we can further improve the accuracy of state-of-the-art retrieval methods by training with explicit region-to-phrase correspondence, but at the same time, they show that accurately inferring this correspondence given an image and a caption remains really challenging.  "}, "references": ["https://www.researchgate.net/publication/268524772_Learning_a_Recurrent_Visual_Representation_for_Image_Caption_Generation", "https://www.researchgate.net/publication/275973890_Language_Models_for_Image_Captioning_The_Quirks_and_What_Works", "https://www.researchgate.net/publication/262272191_Detecting_visual_text", "https://www.researchgate.net/publication/268525836_Long-term_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description", "https://www.researchgate.net/publication/268525230_From_Captions_to_Visual_Concepts_and_Back", "https://www.researchgate.net/publication/221303952_Every_Picture_Tells_a_Story_Generating_Sentences_from_Images", "https://www.researchgate.net/publication/261263530_A_Sentence_Is_Worth_a_Thousand_Pixels", "https://www.researchgate.net/publication/233937091_A_Multi-View_Embedding_Space_for_Modeling_Internet_Images_Tags_and_Their_Semantics", "https://www.researchgate.net/publication/248768967_The_IAPR_TC12_Benchmark_A_New_Evaluation_Resource_for_Visual_Information_Systems", "https://www.researchgate.net/publication/262256563_Framing_Image_Description_as_a_Ranking_Task_Data_Models_and_Evaluation_Metrics", "https://www.researchgate.net/publication/234799793_Cross-caption_coreference_resolution_for_automatic_image_understanding", "https://www.researchgate.net/publication/229057967_Relations_Between_Two_Sets_of_Variates", "https://www.researchgate.net/publication/269339562_Deep_Visual-Semantic_Alignments_for_Generating_Image_Descriptions", "https://www.researchgate.net/publication/263352571_Deep_Fragment_Embeddings_for_Bidirectional_Image_Sentence_Mapping", "https://www.researchgate.net/publication/268155634_Unifying_Visual-Semantic_Embeddings_with_Multimodal_Neural_Language_Models", "https://www.researchgate.net/publication/268988559_Fisher_Vectors_Derived_from_Hybrid_Gaussian-Laplacian_Mixture_Models_for_Image_Annotation", "https://www.researchgate.net/publication/264975559_What_are_you_talking_about_Text-to-Image_Coreference", "https://www.researchgate.net/publication/224254817_Baby_Talk_Understanding_and_Generating_Simple_Image_Descriptions", "https://www.researchgate.net/publication/272194092_Phrase-based_Image_Captioning", "https://www.researchgate.net/publication/262049707_Microsoft_COCO_Common_Objects_in_Context", "https://www.researchgate.net/publication/269935372_Deep_Captioning_with_Multimodal_Recurrent_Neural_Networks_m-RNN", "https://www.researchgate.net/publication/2637882_Using_Decision_Trees_for_Coreference_Resolution", "https://www.researchgate.net/publication/257882504_Distributed_Representations_of_Words_and_Phrases_and_their_Compositionality", "https://www.researchgate.net/publication/221303896_Improving_the_Fisher_Kernel_for_Large-Scale_Image_Classification", "https://www.researchgate.net/publication/262350360_Collecting_image_annotations_using_Amazon's_Mechanical_Turk", "https://www.researchgate.net/publication/265385906_Very_Deep_Convolutional_Networks_for_Large-Scale_Image_Recognition", "https://www.researchgate.net/publication/270878508_Parsing_with_Compositional_Vector_Grammars", "https://www.researchgate.net/publication/2527394_A_Machine_Learning_Approach_to_Coreference_Resolution_of_Noun_Phrases", "https://www.researchgate.net/publication/4351230_Utility_data_annotation_with_Amazon_Mechanical_Turk", "https://www.researchgate.net/publication/268451983_Show_and_Tell_A_Neural_Image_Caption_Generator", "https://www.researchgate.net/publication/272194766_Show_Attend_and_Tell_Neural_Image_Caption_Generation_with_Visual_Attention", "https://www.researchgate.net/publication/224146493_I2T_Image_Parsing_to_Text_Description", "https://www.researchgate.net/publication/278648177_Edge_Boxes_Locating_Object_Proposals_from_Edges"]}