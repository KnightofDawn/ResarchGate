{"cited_in": [], "datas": {"publication_uid": "287249462", "title": "An Empirical Comparison of Neural Architectures for Reinforcement Learning in Partially Observable Environments", "authors": {"1": "Denis Steckelmacher", "2": "Peter Vrancx"}, "abstract": "  ABSTRACT This paper explores the performance of fitted neural Q iteration for reinforcement learning in several partially observable environments, using three recurrent neural network architectures: Long Short-Term Memory, Gated Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of several thousands candidate architectures. A variant of fitted Q iteration, based on Advantage values instead of Q values, is also explored. The results show that GRU performs significantly better than LSTM and MUT1 for most of the problems considered, requiring less training episodes and less CPU time before learning a very good policy. Advantage learning also tends to produce better results.  "}, "references": ["https://www.researchgate.net/publication/2395590_Reinforcement_Learning_with_Long_Short-Term_Memory", "https://www.researchgate.net/publication/220320978_Tree-Based_Batch_Mode_Reinforcement_Learning", "https://www.researchgate.net/publication/2265942_Multi-Player_Residual_Advantage_Learning_With_General_Function_Approximation", "https://www.researchgate.net/publication/13853244_Long_Short-term_Memory", "https://www.researchgate.net/publication/243773520_Reinforcement_learning_with_hidden_states", "https://www.researchgate.net/publication/2310361_Learning_to_Use_Selective_Attention_and_Short-Term_Memory_in_Sequential_Tasks", "https://www.researchgate.net/publication/221112483_Neural_Fitted_Q_Iteration_-_First_Experiences_with_a_Data_Efficient_Neural_Reinforcement_Learning_Method"]}