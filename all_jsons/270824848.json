{"cited_in": ["https://www.researchgate.net/publication/286513006_Video_captioning_with_recurrent_networks_based_on_frame-_and_video-level_features_and_visual_content_classification", "https://www.researchgate.net/publication/280524411_Book2Movie_Aligning_Video_scenes_with_Book_chapters", "https://www.researchgate.net/publication/275897170_Sequence_to_Sequence_--_Video_to_Text", "https://www.researchgate.net/publication/272845653_Recognizing_Fine-Grained_and_Composite_Activities_Using_Hand-Centric_Features_and_Script_Data", "https://www.researchgate.net/publication/286513348_MovieQA_Understanding_Stories_in_Movies_through_Question-Answering", "https://www.researchgate.net/publication/289131075_cvpaperchallenge_in_CVPR2015_-_A_review_of_CVPR2015"], "datas": {"publication_uid": "270824848", "title": "A Dataset for Movie Description", "authors": {"1": "Anna Rohrbach", "4": "Bernt Schiele", "3": "Niket Tandon", "2": "Marcus Rohrbach"}, "abstract": "  ABSTRACT Descriptive video service (DVS) provides linguistic descriptions of movies and allows visually impaired people to follow a movie along with their peers. Such descriptions are by design mainly visual and thus naturally form an interesting data source for computer vision and computational linguistics. In this work we propose a novel dataset which contains transcribed DVS, which is temporally aligned to full length HD movies. In addition we also collected the aligned movie scripts which have been used in prior work and compare the two different sources of descriptions. In total the Movie Description dataset contains a parallel corpus of over 54,000 sentences and video snippets from 72 HD movies. We characterize the dataset by benchmarking different approaches for generating video descriptions. Comparing DVS to scripts, we find that DVS is far more visual and describes precisely what is shown rather than what should happen according to the scripts created prior to movie production.  "}, "references": ["https://www.researchgate.net/publication/220874980_Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation", "https://www.researchgate.net/publication/261479481_A_Thousand_Frames_in_Just_a_Few_Words_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching", "https://www.researchgate.net/publication/262254502_ClausIE_Clause-based_open_information_extraction", "https://www.researchgate.net/publication/220000423_Computer-Vision-Assisted_System_for_Videodescription_Scripting", "https://www.researchgate.net/publication/262242230_YouTube2Text_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition", "https://www.researchgate.net/publication/235937706_Automated_Textual_Descriptions_for_a_Wide_Range_of_Video_Events_with_48_Human_Actions", "https://www.researchgate.net/publication/264084887_LSDA_Large_Scale_Detection_Through_Adaptation", "https://www.researchgate.net/publication/269339562_Deep_Visual-Semantic_Alignments_for_Generating_Image_Descriptions", "https://www.researchgate.net/publication/220874004_Moses_Open_Source_Toolkit_for_Statistical_Machine_Translation", "https://www.researchgate.net/publication/267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks", "https://www.researchgate.net/publication/262174543_Collective_generation_of_natural_image_descriptions", "https://www.researchgate.net/publication/270878729_Context-dependent_Semantic_Parsing_for_Time_Expressions", "https://www.researchgate.net/publication/263002356_Microsoft_COCO_Common_Objects_in_Context", "https://www.researchgate.net/publication/262239422_Midge_generating_image_descriptions_from_computer_vision_detections", "https://www.researchgate.net/publication/221573303_Towards_textually_describing_complex_video_contents_with_audio-visual_concept_classifiers", "https://www.researchgate.net/publication/269636281_Translating_Videos_to_Natural_Language_Using_Deep_Recurrent_Neural_Networks", "https://www.researchgate.net/publication/268451983_Show_and_Tell_A_Neural_Image_Caption_Generator", "https://www.researchgate.net/publication/221362554_SUN_database_Large-scale_scene_recognition_from_abbey_to_zoo", "https://www.researchgate.net/publication/224135986_Automatic_Annotation_of_Human_Actions_in_Video", "https://www.researchgate.net/publication/262049707_Microsoft_COCO_Common_Objects_in_Context", "https://www.researchgate.net/publication/221304391_MovieScript_Alignment_and_Parsing_of_Video_and_Text_Transcription", "https://www.researchgate.net/publication/262320131_Action_Recognition_with_Improved_Trajectories", "https://www.researchgate.net/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database", "https://www.researchgate.net/publication/224579193_Understanding_videos_constructing_plots_learning_a_visually_grounded_storyline_model_from_annotated_videos", "https://www.researchgate.net/publication/263699257_Weakly_Supervised_Action_Labeling_in_Videos_Under_Ordering_Constraints", "https://www.researchgate.net/publication/262205036_An_exact_dual_decomposition_algorithm_for_shallow_semantic_parsing_with_constraints", "https://www.researchgate.net/publication/221429905_Human_Focused_Video_Description", "https://www.researchgate.net/publication/228353665_Extending_VerbNet_with_novel_verb_classes", "https://www.researchgate.net/publication/228917508_Composing_Simple_Image_Descriptions_using_Web-scale_N-grams", "https://www.researchgate.net/publication/29651616_Associating_characters_with_events_in_films", "https://www.researchgate.net/publication/268524772_Learning_a_Recurrent_Visual_Representation_for_Image_Caption_Generation", "https://www.researchgate.net/publication/261100610_Coherent_Multi-Sentence_Video_Description_with_Variable_Level_of_Detail", "https://www.researchgate.net/publication/260302048_Im2Text_Describing_Images_Using_1_Million_Captioned_Photographs", "https://www.researchgate.net/publication/269935372_Deep_Captioning_with_Multimodal_Recurrent_Neural_Networks_m-RNN", "https://www.researchgate.net/publication/268155634_Unifying_Visual-Semantic_Embeddings_with_Multimodal_Neural_Language_Models", "https://www.researchgate.net/publication/266660267_Open_question_answering_over_curated_and_extracted_knowledge_bases", "https://www.researchgate.net/publication/221361842_Learning_realistic_human_actions_from_movies_In_IEEE_CVPR", "https://www.researchgate.net/publication/220816757_VerbNet_overview_extensions_mappings_and_applications", "https://www.researchgate.net/publication/271435947_Translating_Video_Content_to_Natural_Language_Descriptions", "https://www.researchgate.net/publication/220873170_It_Makes_Sense_A_Wide-Coverage_Word_Sense_Disambiguation_System_for_Free_Text", "https://www.researchgate.net/publication/221361820_TVParser_An_automatic_TV_video_parsing_method", "https://www.researchgate.net/publication/262252908_Finding_Actors_and_Actions_in_Movies", "https://www.researchgate.net/publication/221303952_Every_Picture_Tells_a_Story_Generating_Sentences_from_Images", "https://www.researchgate.net/publication/220784058_TRECVID_2010_-_An_Overview_of_the_Goals_Tasks_Data_Evaluation_Mechanisms_and_Metrics", "https://www.researchgate.net/publication/2903260_WordNetSimilarity_-_Measuring_the_Relatedness_of_Concepts", "https://www.researchgate.net/publication/268525230_From_Captions_to_Visual_Concepts_and_Back", "https://www.researchgate.net/publication/220659671_Natural_Language_Description_of_Human_Activities_from_Video_Images_Based_on_Concept_Hierarchy_of_Actions"]}