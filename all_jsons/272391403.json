{"cited_in": ["https://www.researchgate.net/publication/283744301_Creating_training_datasets_for_OCR_in_mobile_device_video_stream", "https://www.researchgate.net/publication/277007762_Textual_Information-Based_Filter_for_Image_Collection_and_Classification", "https://www.researchgate.net/publication/277627450_Textual_Information-Based_Filter_for_Imag_e_Collection_and_Classification"], "datas": {"publication_uid": "272391403", "title": "Semantically-driven automatic creation of training sets for object recognition", "authors": {"1": "Dong Seon Cheng", "4": "Roberta Ferrario", "3": "Nicola Zeni", "2": "Francesco Setti", "5": "Marco Cristani"}, "abstract": "In the object recognition community, much effort has been spent on devising expressive object representations and powerful learning strategies for designing effective classifiers, capable of achieving high accuracy and generalization. In this scenario, the focus on the training sets has been historically weak; by and large, training sets have been generated with a substantial human intervention, requiring considerable time. In this paper, we present a strategy for automatic training set generation. The strategy uses semantic knowledge coming from WordNet, coupled with the statistical power provided by Google Ngram, to select a set of meaningful text strings related to the text class-label (e.g., \u201ccat\u201d), that are subsequently fed into the Google Images search engine, producing sets of images with high training value. Focusing on the classes of different object recognition benchmarks (PASCAL VOC 2012, Caltech-256, ImageNet, GRAZ and OxfordPet), our approach collects novel training images, compared to the ones obtained by exploiting Google Images with the simple text class-label. In particular, we show that the gathered images are better able to capture the different visual facets of a concept, thus encoding in a more successful manner the intra-class variance. As a consequence, training standard classifiers with this data produces performances not too distant from those obtained from the classical hand-crafted training sets. In addition, our datasets generalize well and are stable, that is, they provide similar performances on diverse test datasets. This process does not require manual intervention and is completed in a few hours."}, "references": ["https://www.researchgate.net/publication/262361420_HOGgles_Visualizing_Object_Detection_Features", "https://www.researchgate.net/publication/258884623_Are_all_training_examples_equally_valuable", "https://www.researchgate.net/publication/224254745_Unbiased_look_at_dataset_bias", "https://www.researchgate.net/publication/7211794_One-shot_learning_of_object_categories", "https://www.researchgate.net/publication/221515388_Peekaboom_A_game_for_locating_objects_in_images", "https://www.researchgate.net/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database", "https://www.researchgate.net/publication/244706524_Fast_Accurate_Detection_of_100000_Object_Classes_on_a_Single_Machine", "https://www.researchgate.net/publication/224318376_Informed_visual_search_Combining_attention_and_object_recognition", "https://www.researchgate.net/publication/221368877_On_the_sampling_of_web_images_for_learning_visual_concept_classifiers", "https://www.researchgate.net/publication/238705346_Some_universals_of_grammar_with_special_reference_to_the_order_of_meaningful_elements", "https://www.researchgate.net/publication/220688577_Introduction_To_Modern_Information_Retrieval", "https://www.researchgate.net/publication/267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks", "https://www.researchgate.net/publication/258424423_Visualizing_and_Understanding_Convolutional_Neural_Networks", "https://www.researchgate.net/publication/2935005_Labeling_Images_with_a_Computer_Game", "https://www.researchgate.net/publication/37992037_LabelMe_A_Database_and_Web-Based_Tool_for_Image_Annotation", "https://www.researchgate.net/publication/220659463_The_Pascal_Visual_Object_Classes_VOC_Challenge", "https://www.researchgate.net/publication/37692168_Language_Universals_and_Linguistic_Typology_Syntax_and_Morphology", "https://www.researchgate.net/publication/257409887_DeCAF_A_Deep_Convolutional_Activation_Feature_for_Generic_Visual_Recognition", "https://www.researchgate.net/publication/238321673_'Some_Universals_of_Grammar_with_Particular_Reference_to_the_Order_of_Meaningful_Elements'_in_J", "https://www.researchgate.net/publication/220143292_Curious_George_An_attentive_semantic_robot", "https://www.researchgate.net/publication/220320803_LIBLINEAR_a_library_for_large_linear_classification", "https://www.researchgate.net/publication/225496431_TextonBoost_Joint_Appearance_Shape_and_Context_Modeling_for_Multi-class_Object_Recognition_and_Segmentation", "https://www.researchgate.net/publication/267686145_Do_We_Need_More_Training_Data_or_Better_Models_for_Object_Detection", "https://www.researchgate.net/publication/220728791_Introduction_to_a_Large-Scale_General_Purpose_Ground_Truth_Database_Methodology_Annotation_Tool_and_Benchmarks"]}