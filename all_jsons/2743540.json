{"cited_in": ["https://www.researchgate.net/publication/242025320_Lifelong_Machine_Learning_Systems_Beyond_Learning_Algorithms", "https://www.researchgate.net/publication/221787723_Life-long_Learning_Through_Task_Rehearsal_and_Selective_Knowledge_Transfer", "https://www.researchgate.net/publication/221309167_Requirements_for_Machine_Lifelong_Learning", "https://www.researchgate.net/publication/251397110_Requirements_for_Machine_Lifelong_Learning_TR2005-009_-_November_2005", "https://www.researchgate.net/publication/30758013_Data_mining_fraud_detection_and_mobile_telecommunications_call_pattern_analysis_with_unsupervised_neural_networks", "https://www.researchgate.net/publication/2709599_A_Connectionist_Symbol_Manipulator_That_Discovers_the_Structure_of_Context-Free_Languages", "https://www.researchgate.net/publication/238835966_Neural_Network_Music_Composition_by_Prediction_Exploring_the_Benefits_of_Psychoacoustic_Constraints_and_Multi-scale_Processing", "https://www.researchgate.net/publication/2354971_Using_Periodically_Attentive_Units_to_Extend_the_Temporal_Capacity_of_Simple_Recurrent_Networks", "https://www.researchgate.net/publication/277298117_Bridging_Long_Time_Lags_by_Weight_Guessing_and_Long_Short_Term_Memory", "https://www.researchgate.net/publication/2429143_The_Parallel_Transfer_of_Task_Knowledge_Using_Dynamic_Learning_Rates_Based_on_a_Measure_of_Relatedness", "https://www.researchgate.net/publication/2260639_Large-Scale_Dynamic_Optimization_Using_Teams_of_Reinforcement_Learning_Agents", "https://www.researchgate.net/publication/2409910_CHILD_A_First_Step_Towards_Continual_Learning", "https://www.researchgate.net/publication/13853244_Long_Short-term_Memory", "https://www.researchgate.net/publication/277282943_Recurrent_Neural_Net_Learning_and_Vanishing_Gradient", "https://www.researchgate.net/publication/277294087_ABSTRACT_OF_DISSERTATION_INTEGRATION_OF_PARTIALLY_OBSERVABLE_MARKOV_DECISION_PROCESSES_AND_REINFORCEMENT_LEARNING_FOR_SIMULATED_ROBOT_NAVIGATION", "https://www.researchgate.net/publication/2290449_Bridging_Long_Time_Lags_by_Weight_Guessing_and_Long_Short_Term_Memory", "https://www.researchgate.net/publication/3928861_Evolutionary_Computation_versus_Reinforcement_Learning", "https://www.researchgate.net/publication/220956615_Sequential_Decision_Making_Based_on_Direct_Search", "https://www.researchgate.net/publication/11555985_Schmitt_M_On_the_Complexity_of_Computing_and_Learning_with_multiplicative_Neural_Networks_Neural_Computation_142_241-301", "https://www.researchgate.net/publication/221226181_The_Consolidation_of_Neural_Network_Task_Knowledge", "https://www.researchgate.net/publication/2839938_Gradient_Flow_in_Recurrent_Nets_the_Difficulty_of_Learning_Long-Term_Dependencies", "https://www.researchgate.net/publication/221442226_Sequential_Consolidation_of_Learned_Task_Knowledge", "https://www.researchgate.net/publication/1959540_New_Millennium_AI_and_the_Convergence_of_History", "https://www.researchgate.net/publication/225211154_A_Novel_Connectionist_Network_for_Solving_Long_Time-Lag_Prediction_Tasks", "https://www.researchgate.net/publication/236735712_Better_Generalization_with_Forecasts", "https://www.researchgate.net/publication/262030045_Deep_Learning_in_Neural_Networks_An_Overview", "https://www.researchgate.net/publication/285458567_On_Learning_to_Think_Algorithmic_Information_Theory_for_Novel_Combinations_of_Reinforcement_Learning_Controllers_and_Recurrent_Neural_World_Models"], "datas": {"publication_uid": "2743540", "title": "Learning Sequential Tasks by Incrementally Adding Higher Orders", "authors": {"1": "Mark Ring", "2": "Independent Researcher"}, "abstract": "  ABSTRACT An incremental, higher-order, non-recurrent network combines two properties found to be useful for learning sequential tasks: higherorder connections and incremental introduction of new units. The network adds higher orders when needed by adding new units that dynamically modify connection weights. Since the new units modify the weights at the next time-step with information from the previous step, temporal tasks can be learned without the use of feedback, thereby greatly simplifying training. Furthermore, a theoretically unlimited number of units can be added to reach into the arbitrarily distant past. Experiments with the Reber grammar have demonstrated speedups of two orders of magnitude over recurrent networks. 1 INTRODUCTION Second-order recurrent networks have proven to be very powerful [8], especially when trained using complete back propagation through time [1, 6, 14]. It has also been demonstrated by Fahlman that a recurrent network that incrementally adds nodes during traini...  "}, "references": ["https://www.researchgate.net/publication/243683010_The_utility_driven_dynamic_error_propagation_network", "https://www.researchgate.net/publication/226171158_The_Induction_of_Dynamical_Recognizers", "https://www.researchgate.net/publication/222449846_Finding_Structure_in_Time", "https://www.researchgate.net/publication/226263863_Node_splitting_A_constructive_algorithm_for_feed-forward_neural_networks", "https://www.researchgate.net/publication/230876435_Learning_Internal_Representation_by_Error_Propagation", "https://www.researchgate.net/publication/232581411_Hierarchical_organisation_A_candidate_principle_for_ethology", "https://www.researchgate.net/publication/2430965_The_Recurrent_Cascade-Correlation_Architecture", "https://www.researchgate.net/publication/2622124_Sequence_Learning_with_Incremental_Higher-Order_Neural_Networks", "https://www.researchgate.net/publication/243698906_Finite_State_Automata_and_Simple_Recurrent_Networks"]}