{"cited_in": [], "datas": {"publication_uid": "281447377", "title": "Human Action Recognition using Factorized Spatio-Temporal Convolutional Networks (FstCN)", "authors": {"1": "Lin Sun", "4": "Bertram E.Shi", "3": "D.Y Yeung", "2": "Kui Jia"}, "abstract": "Human actions in video sequences are three-dimensional (3D) spatio-temporal signals characterizing both the visual appearance and motion dynamics of the involved humans and objects. Inspired by the success of convolutional neural networks (CNN) for image classification, recent attempts have been made to learn 3D CNNs for recognizing human actions in videos. However, partly due to the high complexity of training 3D convolution kernels and the need for large quantities of training videos, only limited success has been reported. This has triggered us to investigate in this paper a new deep architecture which can handle 3D signals more effectively. Specifically, we propose factorized spatio-temporal convolutional networks (FstCN) that factorize the original 3D convolution kernel learning as a sequential process of learning 2D spatial kernels in the lower layers (called spatial convolutional layers), followed by learning 1D temporal kernels in the upper layers (called temporal convolutional layers). We introduce a novel transformation and permutation operator to make factorization in FstCN possible. Moreover, to address the issue of sequence alignment, we propose an effective training and inference strategy based on sampling multiple video clips from a given action video sequence. We have tested FstCN on two commonly used benchmark datasets (UCF-101 and HMDB-51). Without using auxiliary training videos to boost the performance, FstCN outperforms existing CNN based methods and achieves comparable performance with a recent method that benefits from using auxiliary training videos."}, "references": ["https://www.researchgate.net/publication/261440156_Neural_Codes_for_Image_Retrieval", "https://www.researchgate.net/publication/240308781_Learning_Hierarchical_Features_for_Scene_Labeling", "https://www.researchgate.net/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database", "https://www.researchgate.net/publication/258374356_Rich_Feature_Hierarchies_for_Accurate_Object_Detection_and_Semantic_Segmentation", "https://www.researchgate.net/publication/5911536_Actions_as_Space-Time_Shapes", "https://www.researchgate.net/publication/221110709_A_Biologically_Inspired_System_for_Action_Recognition", "https://www.researchgate.net/publication/221346417_3D_Convolutional_Neural_Networks_for_Human_Action_Recognition", "https://www.researchgate.net/publication/221361651_Human_action_recognition_using_Local_Spatio-Temporal_Discriminant_Embedding", "https://www.researchgate.net/publication/264979485_Caffe_Convolutional_Architecture_for_Fast_Feature_Embedding", "https://www.researchgate.net/publication/221259643_A_Spatio-Temporal_Descriptor_Based_on_3D-Gradients", "https://www.researchgate.net/publication/267960550_ImageNet_Classification_with_Deep_Convolutional_Neural_Networks", "https://www.researchgate.net/publication/221111664_HMDB51_A_Large_Video_Database_for_Human_Motion_Recognition", "https://www.researchgate.net/publication/4038396_Space-time_interest_points", "https://www.researchgate.net/publication/221361842_Learning_realistic_human_actions_from_movies_In_IEEE_CVPR", "https://www.researchgate.net/publication/2985446_Gradient-based_learning_applied_to_document_recognition_Proc_IEEE", "https://www.researchgate.net/publication/268391589_Fully_Convolutional_Networks_for_Semantic_Segmentation", "https://www.researchgate.net/publication/3816624_Object_Recognition_from_Local_Scale-Invariant_Features", "https://www.researchgate.net/publication/3176845_Performance_Trade-offs_in_Dynamic_Time_Warping_Algorithms_for_Isolated_Word_Recognition", "https://www.researchgate.net/publication/220659676_Unsupervised_Learning_of_Human_Action_Categories_Using_Spatial-Temporal_Words", "https://www.researchgate.net/publication/262452311_Bag_of_Visual_Words_and_Fusion_Methods_for_Action_Recognition_Comprehensive_Study_and_Good_Practice", "https://www.researchgate.net/publication/221260184_Bag_of_Optical_Flow_Volumes_for_Image_Sequence_Recognition", "https://www.researchgate.net/publication/4090526_Recognizing_human_actions_A_local_SVM_approach", "https://www.researchgate.net/publication/233815759_UCF101_A_Dataset_of_101_Human_Actions_Classes_From_Videos_in_The_Wild", "https://www.researchgate.net/publication/265787949_Going_Deeper_with_Convolutions", "https://www.researchgate.net/publication/241858646_Visualizing_High-Dimensional_Data_Using_t-SNE", "https://www.researchgate.net/publication/262320131_Action_Recognition_with_Improved_Trajectories", "https://www.researchgate.net/publication/221259353_Evaluation_of_Local_Spatio-temporal_Features_for_Action_Recognition", "https://www.researchgate.net/publication/221361902_Action_Recognition_using_Exemplar-based_Embedding", "https://www.researchgate.net/publication/224309574_Robust_Face_Recognition_via_Sparse_Representation", "https://www.researchgate.net/publication/258424423_Visualizing_and_Understanding_Convolutional_Neural_Networks", "https://www.researchgate.net/publication/51539290_Slow_Feature_Analysis_for_Human_Action_Recognition"]}