{"cited_in": ["https://www.researchgate.net/publication/286513348_MovieQA_Understanding_Stories_in_Movies_through_Question-Answering", "https://www.researchgate.net/publication/291166923_Under_review_as_a_conference_paper_at_ICLR_2016_STORIES_IN_THE_EYE_CONTEXTUAL_VISUAL_IN-_TERACTIONS_FOR_EFFICIENT_VIDEO_TO_LANGUAGE_TRANSLATION", "https://www.researchgate.net/publication/283658947_VideoStory_Embeddings_Recognize_Events_when_Examples_are_Scarce", "https://www.researchgate.net/publication/270824848_A_Dataset_for_Movie_Description", "https://www.researchgate.net/publication/280524411_Book2Movie_Aligning_Video_scenes_with_Book_chapters", "https://www.researchgate.net/publication/283638316_OSVC_-_Open_Short_Video_Collection_10", "https://www.researchgate.net/publication/261100610_Coherent_Multi-Sentence_Video_Description_with_Variable_Level_of_Detail", "https://www.researchgate.net/publication/262242230_YouTube2Text_Recognizing_and_Describing_Arbitrary_Activities_Using_Semantic_Hierarchies_and_Zero-Shot_Recognition", "https://www.researchgate.net/publication/261277330_Are_Actor_and_Action_Semantics_Retained_in_Video_Supervoxel_Segmentation", "https://www.researchgate.net/publication/271435947_Translating_Video_Content_to_Natural_Language_Descriptions", "https://www.researchgate.net/publication/269294912_A_maximal_figure-of-merit_learning_approach_to_maximizing_mean_average_precision_with_deep_neural_network_based_classifiers", "https://www.researchgate.net/publication/267271971_Compositional_Structure_Learning_for_Action_Understanding", "https://www.researchgate.net/publication/268525836_Long-term_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description", "https://www.researchgate.net/publication/269636281_Translating_Videos_to_Natural_Language_Using_Deep_Recurrent_Neural_Networks", "https://www.researchgate.net/publication/273818079_Robot_Learning_Manipulation_Action_Plans_by_Watching_Unconstrained_Videos_from_the_World_Wide_Web", "https://www.researchgate.net/publication/275032121_Grasp_Type_Revisited_A_Modern_Perspective_on_A_Classical_Feature_for_Vision", "https://www.researchgate.net/publication/272845653_Recognizing_Fine-Grained_and_Composite_Activities_Using_Hand-Centric_Features_and_Script_Data", "https://www.researchgate.net/publication/271726354_A_Framework_for_Creating_Natural_Language_Descriptions_of_Video_Streams", "https://www.researchgate.net/publication/277959204_Sentence_Directed_Video_Object_Codetection", "https://www.researchgate.net/publication/279458879_Unsupervised_Semantic_Parsing_of_Video_Collections", "https://www.researchgate.net/publication/285385176_On_the_use_of_commonsense_ontology_for_multimedia_event_recounting"], "datas": {"publication_uid": "261479481", "title": "A Thousand Frames in Just a Few Words: Lingual Description of Videos through Latent Topics and Sparse Object Stitching", "authors": {"1": "Pradipto Das", "4": "Jason Corso", "3": "Richard F. Doell", "2": "Chenliang Xu"}, "abstract": "The problem of describing images through natural language has gained importance in the computer vision community. Solutions to image description have either focused on a top-down approach of generating language through combinations of object detections and language models or bottom-up propagation of keyword tags from training images to test images through probabilistic or nearest neighbor techniques. In contrast, describing videos with natural language is a less studied problem. In this paper, we combine ideas from the bottom-up and top-down approaches to image description and propose a method for video description that captures the most relevant contents of a video in a natural language description. We propose a hybrid system consisting of a low level multimodal latent topic model for initial keyword annotation, a middle level of concept detectors and a high level module to produce final lingual descriptions. We compare the results of our system to human descriptions in both short and long forms on two datasets, and demonstrate that final system output has greater agreement with the human descriptions than any single level."}, "references": ["https://www.researchgate.net/publication/221363125_Simultaneous_image_classification_and_annotation", "https://www.researchgate.net/publication/224135977_TagProp_Discriminative_Metric_Learning_in_Nearest_Neighbor_Models_for_Image_Auto-Annotation", "https://www.researchgate.net/publication/220319974_Latent_Dirichlet_Allocation", "https://www.researchgate.net/publication/262164245_Script_Data_for_Attribute-Based_Recognition_of_Composite_Activities", "https://www.researchgate.net/publication/224577638_Evaluating_color_descriptors_for_object_and_scene_recognition_IEEE_Trans_Pattern_Anal_Mach_Intell", "https://www.researchgate.net/publication/221362216_Topic_regression_multi-modal_Latent_Dirichlet_Allocation_for_image_annotation", "https://www.researchgate.net/publication/221303908_A_New_Baseline_for_Image_Annotation", "https://www.researchgate.net/publication/221618817_Rethinking_LDA_Why_priors_matter", "https://www.researchgate.net/publication/262274329_Translating_related_words_to_videos_and_back_through_latent_topics", "https://www.researchgate.net/publication/224254817_Baby_Talk_Understanding_and_Generating_Simple_Image_Descriptions", "https://www.researchgate.net/publication/221303952_Every_Picture_Tells_a_Story_Generating_Sentences_from_Images", "https://www.researchgate.net/publication/220784058_TRECVID_2010_-_An_Overview_of_the_Goals_Tasks_Data_Evaluation_Mechanisms_and_Metrics", "https://www.researchgate.net/publication/224254745_Unbiased_look_at_dataset_bias", "https://www.researchgate.net/publication/220946975_Comparing_Automatic_and_Human_Evaluation_of_NLG_Systems", "https://www.researchgate.net/publication/221012945_Corpus-Guided_Sentence_Generation_of_Natural_Images", "https://www.researchgate.net/publication/2588204_BLEU_a_Method_for_Automatic_Evaluation_of_Machine_Translation", "https://www.researchgate.net/publication/230668339_Action_Bank_A_High-Level_Representation_of_Activity_in_Video", "https://www.researchgate.net/publication/257672284_Efficiently_Scaling_up_Crowdsourced_Video_Annotation", "https://www.researchgate.net/publication/224890498_Automatic_Evaluation_of_Summaries_Using_n-gram_Co-occurrence_Statistics", "https://www.researchgate.net/publication/221111384_A_Markov_Clustering_Topic_Model_for_Mining_Behaviour_in_Video", "https://www.researchgate.net/publication/262350360_Collecting_image_annotations_using_Amazon's_Mechanical_Turk", "https://www.researchgate.net/publication/221410219_Lecture_Notes_in_Computer_Science", "https://www.researchgate.net/publication/261019619_Recognition_using_visual_phrases", "https://www.researchgate.net/publication/221259643_A_Spatio-Temporal_Descriptor_Based_on_3D-Gradients", "https://www.researchgate.net/publication/2950087_Modeling_Annotated_Data", "https://www.researchgate.net/publication/221429991_Towards_coherent_natural_language_description_of_video_streams", "https://www.researchgate.net/publication/220416605_Graphical_Models_Exponential_Families_and_Variational_Inference", "https://www.researchgate.net/publication/221318629_Topic_models_for_semantics-preserving_video_compression", "https://www.researchgate.net/publication/4301633_Spatially_Coherent_Latent_Topic_Model_for_Concurrent_Segmentation_and_Classification_of_Objects_and_Scenes", "https://www.researchgate.net/publication/286127111_Generating_natural-language_video_descriptions_using_text-mined_knowledge", "https://www.researchgate.net/publication/220817220_Topic_Models_for_Image_Annotation_and_Text_Illustration", "https://www.researchgate.net/publication/220659463_The_Pascal_Visual_Object_Classes_VOC_Challenge", "https://www.researchgate.net/publication/45200127_Object_Detection_with_Discriminatively_Trained_Part-Based_Models", "https://www.researchgate.net/publication/284177509_TRECVID_2012_-_An_Overview_of_the_Goals_Tasks_Data_Evaluation_Mechanisms_and_Metrics"]}