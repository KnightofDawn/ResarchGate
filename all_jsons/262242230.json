{"cited_in": ["https://www.researchgate.net/publication/282252483_Objects2action_Classifying_and_localizing_actions_without_any_video_example", "https://www.researchgate.net/publication/291166923_Under_review_as_a_conference_paper_at_ICLR_2016_STORIES_IN_THE_EYE_CONTEXTUAL_VISUAL_IN-_TERACTIONS_FOR_EFFICIENT_VIDEO_TO_LANGUAGE_TRANSLATION", "https://www.researchgate.net/publication/282810120_jainICCV15Objects2action", "https://www.researchgate.net/publication/270824848_A_Dataset_for_Movie_Description", "https://www.researchgate.net/publication/275897170_Sequence_to_Sequence_--_Video_to_Text", "https://www.researchgate.net/publication/269636281_Translating_Videos_to_Natural_Language_Using_Deep_Recurrent_Neural_Networks", "https://www.researchgate.net/publication/266376838_A_Multi-World_Approach_to_Question_Answering_about_Real-World_Scenes_based_on_Uncertain_Input", "https://www.researchgate.net/publication/263352348_VideoSET_Video_Summary_Evaluation_through_Text", "https://www.researchgate.net/publication/286746886_Topic_hierarchy_in_social_networks", "https://www.researchgate.net/publication/275407223_VideoStory_A_New_Multimedia_Embedding_for_Few-Example_Recognition_and_Translation_of_Events", "https://www.researchgate.net/publication/268525836_Long-term_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description", "https://www.researchgate.net/publication/272845653_Recognizing_Fine-Grained_and_Composite_Activities_Using_Hand-Centric_Features_and_Script_Data", "https://www.researchgate.net/publication/273157812_Using_Descriptive_Video_Services_to_Create_a_Large_Data_Source_for_Video_Annotation_Research", "https://www.researchgate.net/publication/281789969_A_Compositional_Framework_for_Grounding_Language_Inference_Generation_and_Acquisition_in_Video", "https://www.researchgate.net/publication/271726354_A_Framework_for_Creating_Natural_Language_Descriptions_of_Video_Streams", "https://www.researchgate.net/publication/275897099_VQA_Visual_Question_Answering", "https://www.researchgate.net/publication/277335193_A_Multi-scale_Multiple_Instance_Video_Description_Network", "https://www.researchgate.net/publication/277959204_Sentence_Directed_Video_Object_Codetection", "https://www.researchgate.net/publication/282831216_Large_Scale_Retrieval_and_Generation_of_Image_Descriptions", "https://www.researchgate.net/publication/276084508_Semantic_human_activity_recognition_A_literature_review", "https://www.researchgate.net/publication/284097355_Trainable_performance_upper_bounds_for_image_and_video_captioning", "https://www.researchgate.net/publication/283904877_A_Review_of_Human_Activity_Recognition_Methods", "https://www.researchgate.net/publication/285385176_On_the_use_of_commonsense_ontology_for_multimedia_event_recounting"], "datas": {"publication_uid": "262242230", "title": "YouTube2Text: Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition", "authors": {"2": "Niveda Krishnamoorthy", "6": "Trevor Darrell", "1": "Sergio Guadarrama", "4": "Subhashini Venugopalan", "3": "Girish Malkarnenkar", "5": "Raymond Mooney"}, "abstract": "  ABSTRACT Despite a recent push towards large-scale object recognition, activity recognition remains limited to narrow domains and small vocabularies of actions. In this paper, we tackle the challenge of recognizing and describing activities ``in-the-wild''. We present a solution that takes a short video clip and outputs a brief sentence that sums up the main activity in the video, such as the actor, the action and its object. Unlike previous work, our approach works on out-of-domain actions: it does not require training videos of the exact activity. If it cannot find an accurate prediction for a pre-trained model, it finds a less specific answer that is also plausible from a pragmatic standpoint. We use semantic hierarchies learned from the data to help to choose an appropriate level of generalization, and priors learned from web-scale natural language corpora to penalize unlikely combinations of actors/actions/objects, we also use a web-scale language model to ``fill in'' novel verbs, i.e. when the verb does not appear in the training set. We evaluate our method on a large YouTube corpus and demonstrate it is able to generate short sentence descriptions of video clips better than baseline approaches.  "}, "references": ["https://www.researchgate.net/publication/228715647_LIBSVM_A_library_for_support_vector_machines", "https://www.researchgate.net/publication/220874980_Collecting_Highly_Parallel_Data_for_Paraphrase_Evaluation", "https://www.researchgate.net/publication/261121847_Hedging_your_bets_Optimizing_accuracy-specificity_trade-offs_in_large_scale_visual_recognition", "https://www.researchgate.net/publication/45200127_Object_Detection_with_Discriminatively_Trained_Part-Based_Models", "https://www.researchgate.net/publication/262289973_Describing_video_contents_in_natural_language", "https://www.researchgate.net/publication/220659671_Natural_Language_Description_of_Human_Activities_from_Video_Images_Based_on_Concept_Hierarchy_of_Actions", "https://www.researchgate.net/publication/224254817_Baby_Talk_Understanding_and_Generating_Simple_Image_Descriptions", "https://www.researchgate.net/publication/4351223_SAVE_A_framework_for_semantic_annotation_of_visual_events", "https://www.researchgate.net/publication/221620611_Object_Bank_A_High-Level_Image_Representation_for_Scene_Classification_Semantic_Feature_Sparsification", "https://www.researchgate.net/publication/2473382_Distributional_Clustering_Of_English_Words", "https://www.researchgate.net/publication/257334067_Recognizing_50_human_action_categories_of_web_videos", "https://www.researchgate.net/publication/51025101_Action_Recognition_by_Dense_Trajectories", "https://www.researchgate.net/publication/221012945_Corpus-Guided_Sentence_Generation_of_Natural_Images", "https://www.researchgate.net/publication/224146493_I2T_Image_Parsing_to_Text_Description", "https://www.researchgate.net/publication/223967108_Video_In_Sentences_Out", "https://www.researchgate.net/publication/2903260_WordNetSimilarity_-_Measuring_the_Relatedness_of_Concepts", "https://www.researchgate.net/publication/221303952_Every_Picture_Tells_a_Story_Generating_Sentences_from_Images", "https://www.researchgate.net/publication/4090526_Recognizing_human_actions_A_local_SVM_approach", "https://www.researchgate.net/publication/230585379_Verbs_Semantics_and_Lexical_Selection", "https://www.researchgate.net/publication/254462598_Beyond_audio_and_video_retrieval_Towards_Multimedia_Summarization", "https://www.researchgate.net/publication/37992037_LabelMe_A_Database_and_Web-Based_Tool_for_Image_Annotation", "https://www.researchgate.net/publication/220659463_The_Pascal_Visual_Object_Classes_VOC_Challenge", "https://www.researchgate.net/publication/220765837_Video2Text_Learning_to_Annotate_Video_Content", "https://www.researchgate.net/publication/228917508_Composing_Simple_Image_Descriptions_using_Web-scale_N-grams", "https://www.researchgate.net/publication/221111814_Retrieving_actions_in_movies", "https://www.researchgate.net/publication/261479481_A_Thousand_Frames_in_Just_a_Few_Words_Lingual_Description_of_Videos_through_Latent_Topics_and_Sparse_Object_Stitching"]}