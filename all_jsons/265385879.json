{"cited_in": ["https://www.researchgate.net/publication/287853493_Backbone_Language_Modeling_for_Constrained_Natural_Language_Generation", "https://www.researchgate.net/publication/283280022_On_End-to-End_Program_Generation_from_User_Intention_by_Deep_Neural_Networks", "https://www.researchgate.net/publication/281084636_Depth-Gated_Recurrent_Neural_Networks", "https://www.researchgate.net/publication/279310212_Ask_Me_Anything_Dynamic_Memory_Networks_for_Natural_Language_Processing", "https://www.researchgate.net/publication/275897170_Sequence_to_Sequence_--_Video_to_Text", "https://www.researchgate.net/publication/268525836_Long-term_Recurrent_Convolutional_Networks_for_Visual_Recognition_and_Description", "https://www.researchgate.net/publication/265252627_Neural_Machine_Translation_by_Jointly_Learning_to_Align_and_Translate", "https://www.researchgate.net/publication/266477935_Not_All_Neural_Embeddings_are_Born_Equal", "https://www.researchgate.net/publication/270454410_Sequence_Modeling_using_Gated_Recurrent_Neural_Networks", "https://www.researchgate.net/publication/277959376_Visualizing_and_Understanding_Recurrent_Networks", "https://www.researchgate.net/publication/279864685_Describing_Multimedia_Content_Using_Attention-Based_Encoder-Decoder_Networks", "https://www.researchgate.net/publication/281895813_Guiding_Long-Short_Term_Memory_for_Image_Caption_Generation", "https://www.researchgate.net/publication/283761501_Larger-Context_Language_Modelling", "https://www.researchgate.net/publication/284476210_Unitary_Evolution_Recurrent_Neural_Networks", "https://www.researchgate.net/publication/284579100_Session-based_Recommendations_with_Recurrent_Neural_Networks"], "datas": {"publication_uid": "265385879", "title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "authors": {"1": "Kyunghyun Cho", "4": "Y. Bengio", "3": "Dzmitry Bahdanau", "2": "Bart van Merri\u00ebnboer"}, "abstract": "Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically."}, "references": ["https://www.researchgate.net/publication/13853244_Long_Short-term_Memory", "https://www.researchgate.net/publication/233409624_Sequence_Transduction_with_Recurrent_Neural_Networks", "https://www.researchgate.net/publication/262877889_Learning_Phrase_Representations_using_RNN_Encoder-Decoder_for_Statistical_Machine_Translation", "https://www.researchgate.net/publication/221012675_Domain_Adaptation_via_Pseudo_In-Domain_Data_Selection"]}